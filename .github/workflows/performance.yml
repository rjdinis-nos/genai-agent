name: Performance Monitoring

on:
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to test'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production
      duration:
        description: 'Test duration in minutes'
        required: true
        default: '5'
        type: string

jobs:
  performance-test:
    name: Performance Test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        pip install locust requests
        
    - name: Create performance test script
      run: |
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import json
        import io
        
        class FastAPIUser(HttpUser):
            wait_time = between(1, 3)
            
            def on_start(self):
                """Setup for each user"""
                pass
                
            @task(3)
            def test_docs_endpoint(self):
                """Test the docs endpoint"""
                self.client.get("/docs")
                
            @task(2)
            def test_download_endpoint(self):
                """Test the download endpoint"""
                payload = {
                    "url": "https://httpbin.org/json",
                    "filename": "test.json"
                }
                self.client.post("/download", json=payload)
                
            @task(1)
            def test_summarize_endpoint(self):
                """Test the summarize endpoint with a small PDF"""
                # Create a minimal PDF-like content for testing
                files = {
                    'file': ('test.pdf', b'%PDF-1.4 test content', 'application/pdf')
                }
                self.client.post("/summarize", files=files)
        EOF
        
    - name: Run performance tests
      run: |
        ENVIRONMENT="${{ github.event.inputs.environment || 'staging' }}"
        DURATION="${{ github.event.inputs.duration || '5' }}"
        
        case $ENVIRONMENT in
          "staging")
            HOST="https://staging-api.yourdomain.com"
            ;;
          "production")
            HOST="https://api.yourdomain.com"
            ;;
          *)
            HOST="http://localhost:8000"
            ;;
        esac
        
        echo "🚀 Running performance tests against $HOST for ${DURATION} minutes..."
        
        # Start local server if testing locally
        if [ "$HOST" = "http://localhost:8000" ]; then
          echo "Starting local server for testing..."
          echo "GEMINI_API_KEY=test_key_for_perf" > .env
          .docker/run.sh &
          sleep 10
        fi
        
        # Run Locust performance tests
        locust --host=$HOST \
               --users=10 \
               --spawn-rate=2 \
               --run-time=${DURATION}m \
               --html=performance-report.html \
               --csv=performance-results \
               --headless
               
    - name: Analyze performance results
      run: |
        echo "📊 Analyzing performance results..."
        
        if [ -f performance-results_stats.csv ]; then
          echo "Performance Statistics:"
          cat performance-results_stats.csv
          
          # Check for performance thresholds
          AVG_RESPONSE_TIME=$(awk -F',' 'NR==2 {print $7}' performance-results_stats.csv)
          FAILURE_RATE=$(awk -F',' 'NR==2 {print $10}' performance-results_stats.csv)
          
          echo "Average Response Time: ${AVG_RESPONSE_TIME}ms"
          echo "Failure Rate: ${FAILURE_RATE}%"
          
          # Set performance thresholds
          if (( $(echo "$AVG_RESPONSE_TIME > 2000" | bc -l) )); then
            echo "⚠️ Average response time exceeds 2000ms threshold"
            echo "PERFORMANCE_WARNING=true" >> $GITHUB_ENV
          fi
          
          if (( $(echo "$FAILURE_RATE > 5" | bc -l) )); then
            echo "❌ Failure rate exceeds 5% threshold"
            echo "PERFORMANCE_FAILURE=true" >> $GITHUB_ENV
          fi
        fi
        
    - name: Upload performance report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-report
        path: |
          performance-report.html
          performance-results_*.csv
          
    - name: Create performance issue
      if: env.PERFORMANCE_FAILURE == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const title = '🐌 Performance Degradation Detected';
          const body = `
          ## 🐌 Performance Alert
          
          Performance monitoring has detected degradation in the application.
          
          ### Issues Detected:
          - High failure rate (>5%)
          - Response times may be elevated
          
          ### Environment: ${{ github.event.inputs.environment || 'staging' }}
          
          ### Action Required:
          1. Review the performance report in the workflow artifacts
          2. Investigate potential causes (database, external APIs, resource limits)
          3. Consider scaling or optimization measures
          4. Re-run performance tests after fixes
          
          **This issue was automatically created by the performance monitoring workflow.**
          `;
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: title,
            body: body,
            labels: ['performance', 'automated', 'bug']
          });

  load-test:
    name: Load Test
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Run load test with Artillery
      run: |
        npm install -g artillery
        
        # Create Artillery configuration
        cat > artillery-config.yml << 'EOF'
        config:
          target: 'http://localhost:8000'
          phases:
            - duration: 60
              arrivalRate: 5
              name: "Warm up"
            - duration: 120
              arrivalRate: 10
              name: "Ramp up load"
            - duration: 300
              arrivalRate: 20
              name: "Sustained load"
        scenarios:
          - name: "API Load Test"
            weight: 100
            flow:
              - get:
                  url: "/docs"
              - post:
                  url: "/download"
                  json:
                    url: "https://httpbin.org/json"
                    filename: "load-test.json"
        EOF
        
        # Start application
        echo "GEMINI_API_KEY=test_key_for_load" > .env
        .docker/run.sh &
        sleep 15
        
        # Run load test
        artillery run artillery-config.yml --output load-test-results.json
        
        # Generate report
        artillery report load-test-results.json --output load-test-report.html
        
    - name: Upload load test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: load-test-results
        path: |
          load-test-report.html
          load-test-results.json

  benchmark:
    name: Benchmark Comparison
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install benchmarking tools
      run: |
        pip install pytest-benchmark
        
    - name: Create benchmark tests
      run: |
        mkdir -p benchmarks
        cat > benchmarks/test_benchmarks.py << 'EOF'
        import pytest
        import requests
        import time
        from unittest.mock import Mock, patch
        import sys
        import os
        
        # Add backend to path
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'backend'))
        
        from main import app
        from fastapi.testclient import TestClient
        
        client = TestClient(app)
        
        class TestBenchmarks:
            def test_docs_endpoint_benchmark(self, benchmark):
                """Benchmark the docs endpoint"""
                result = benchmark(client.get, "/docs")
                assert result.status_code == 200
                
            @patch('backend.main.requests.get')
            def test_download_endpoint_benchmark(self, mock_get, benchmark):
                """Benchmark the download endpoint"""
                mock_response = Mock()
                mock_response.status_code = 200
                mock_response.headers = {'content-type': 'application/json'}
                mock_response.content = b'{"test": "data"}'
                mock_get.return_value = mock_response
                
                def download_request():
                    return client.post("/download", json={
                        "url": "https://example.com/test.json",
                        "filename": "benchmark.json"
                    })
                
                result = benchmark(download_request)
                assert result.status_code == 200
                
            @patch.dict(os.environ, {'GEMINI_API_KEY': 'test_key'})
            @patch('backend.main.genai.GenerativeModel')
            @patch('backend.main.PyPDF2.PdfReader')
            def test_summarize_endpoint_benchmark(self, mock_pdf, mock_genai, benchmark):
                """Benchmark the summarize endpoint"""
                # Mock PDF reader
                mock_page = Mock()
                mock_page.extract_text.return_value = "Test PDF content for benchmarking"
                mock_pdf.return_value.pages = [mock_page]
                
                # Mock Gemini AI
                mock_model = Mock()
                mock_response = Mock()
                mock_response.text = "Test summary for benchmarking"
                mock_model.generate_content.return_value = mock_response
                mock_genai.return_value = mock_model
                
                def summarize_request():
                    return client.post("/summarize", files={
                        "file": ("test.pdf", b"fake pdf content", "application/pdf")
                    })
                
                result = benchmark(summarize_request)
                assert result.status_code == 200
        EOF
        
    - name: Run benchmarks
      run: |
        echo "GEMINI_API_KEY=test_key_for_benchmark" > .env
        python -m pytest benchmarks/ --benchmark-only --benchmark-json=benchmark-results.json
        
    - name: Compare with previous benchmarks
      run: |
        echo "📊 Benchmark Results:"
        if [ -f benchmark-results.json ]; then
          python -c "
          import json
          with open('benchmark-results.json') as f:
              data = json.load(f)
          for benchmark in data['benchmarks']:
              name = benchmark['name']
              mean = benchmark['stats']['mean']
              stddev = benchmark['stats']['stddev']
              print(f'{name}: {mean:.4f}s ± {stddev:.4f}s')
          "
        fi
        
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-results
        path: benchmark-results.json

  resource-monitoring:
    name: Resource Monitoring
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Monitor resource usage
      run: |
        echo "📊 Monitoring resource usage..."
        
        # Start application
        echo "GEMINI_API_KEY=test_key_for_monitoring" > .env
        .docker/run.sh &
        APP_PID=$!
        sleep 10
        
        # Monitor for 2 minutes
        echo "timestamp,cpu_percent,memory_mb,disk_io" > resource-usage.csv
        
        for i in {1..24}; do
          TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M:%S")
          
          # Get container stats
          CONTAINER_ID=$(docker ps --filter "name=fastapi-app" --format "{{.ID}}")
          if [ -n "$CONTAINER_ID" ]; then
            STATS=$(docker stats --no-stream --format "{{.CPUPerc}},{{.MemUsage}}" $CONTAINER_ID)
            CPU_PERCENT=$(echo $STATS | cut -d',' -f1 | sed 's/%//')
            MEMORY_USAGE=$(echo $STATS | cut -d',' -f2 | cut -d'/' -f1 | sed 's/MiB//')
            
            echo "$TIMESTAMP,$CPU_PERCENT,$MEMORY_USAGE,0" >> resource-usage.csv
          fi
          
          sleep 5
        done
        
        # Analyze resource usage
        echo "📈 Resource Usage Analysis:"
        python3 -c "
        import csv
        with open('resource-usage.csv', 'r') as f:
            reader = csv.DictReader(f)
            cpu_values = []
            memory_values = []
            for row in reader:
                if row['cpu_percent'] and row['memory_mb']:
                    cpu_values.append(float(row['cpu_percent']))
                    memory_values.append(float(row['memory_mb']))
            
            if cpu_values and memory_values:
                avg_cpu = sum(cpu_values) / len(cpu_values)
                max_cpu = max(cpu_values)
                avg_memory = sum(memory_values) / len(memory_values)
                max_memory = max(memory_values)
                
                print(f'Average CPU: {avg_cpu:.2f}%')
                print(f'Peak CPU: {max_cpu:.2f}%')
                print(f'Average Memory: {avg_memory:.2f}MB')
                print(f'Peak Memory: {max_memory:.2f}MB')
                
                # Check thresholds
                if max_cpu > 80:
                    print('⚠️ High CPU usage detected')
                if max_memory > 400:
                    print('⚠️ High memory usage detected')
        "
        
    - name: Upload resource monitoring results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: resource-monitoring
        path: resource-usage.csv
